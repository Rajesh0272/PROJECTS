{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97851bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline    \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import precision_score, recall_score, plot_confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c06cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\M RAJESH\\Desktop\\Ekeeda DS\\spam.csv\", encoding='ISO-8859-1') #reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dabd38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #printing 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b99982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() \n",
    "#information of dataset to understand well. it gives information regarding non nulll values, datatype,total entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2d8ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1               0\n",
       "v2               0\n",
       "Unnamed: 2    5522\n",
       "Unnamed: 3    5560\n",
       "Unnamed: 4    5566\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() #checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd8e11ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape #printing rows and columns of df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1efa46",
   "metadata": {},
   "source": [
    "As we can see the null values of unnamed 2,3,4 columns are much high, so we can delete those columns from df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efc5100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " bt not his girlfrnd... G o o d n i g h t . . .@\"                                                                                                   3\n",
       " PO Box 5249                                                                                                                                        2\n",
       "this wont even start........ Datz confidence..\"                                                                                                     2\n",
       "GN                                                                                                                                                  2\n",
       " don't miss ur best life for anything... Gud nyt...\"                                                                                                2\n",
       " but dont try to prove it..\\\" .Gud noon....\"                                                                                                        2\n",
       " Gud night....\"                                                                                                                                     1\n",
       " like you are the KING\\\"...! OR \\\"Walk like you Dont care                                                                                           1\n",
       " HAD A COOL NYTHO                                                                                                                                   1\n",
       " PO Box 1146 MK45 2WT (2/3)\"                                                                                                                        1\n",
       " \\\"It is d wonderful fruit that a tree gives when it is being hurt by a stone.. Good night......\"                                                   1\n",
       " we made you hold all the weed\\\"\"                                                                                                                   1\n",
       " its a miracle to Love a person who can't Love anyone except U...\\\" Gud nyt...\"                                                                     1\n",
       " hopeSo hunny. i amnow feelin ill & ithink i may have tonsolitusaswell! damn iam layin in bedreal bored. lotsof luv me xxxx\\\"\"                      1\n",
       " that's the tiny street where the parking lot is\"                                                                                                   1\n",
       "PROBPOP IN & CU SATTHEN HUNNY 4BREKKIE! LOVE JEN XXX. PSXTRA LRG PORTIONS 4 ME PLEASE \\\"\"                                                           1\n",
       " SHE SHUDVETOLD U. DID URGRAN KNOW?NEWAY                                                                                                            1\n",
       " GOD said                                                                                                                                           1\n",
       " always give response 2 who cares 4 U\\\"... Gud night..swt dreams..take care\"                                                                        1\n",
       " HOPE UR OK... WILL GIVE U A BUZ WEDLUNCH. GO OUTSOMEWHERE 4 ADRINK IN TOWN..CUD GO 2WATERSHD 4 A BIT? PPL FROMWRK WILL BTHERE. LOVE PETEXXX.\\\"\"    1\n",
       " b'coz nobody will fight for u. Only u &amp; u have to fight for ur self &amp; win the battle. -VIVEKANAND- G 9t.. SD..\"                            1\n",
       "DEVIOUSBITCH.ANYWAY                                                                                                                                 1\n",
       " but watever u shared should be true\\\"....\"                                                                                                         1\n",
       " Dont Come Near My Body..!! Bcoz My Hands May Not Come 2 Wipe Ur Tears Off That Time..!Gud ni8\"                                                     1\n",
       " but dont try to prove\\\" ..... Gud mrng...\"                                                                                                         1\n",
       " the toughest is acting Happy with all unspoken pain inside..\\\"\"                                                                                    1\n",
       " HOWU DOIN? FOUNDURSELF A JOBYET SAUSAGE?LOVE JEN XXX\\\"\"                                                                                            1\n",
       " wanted to say hi. HI!!!\\\" Stop? Send STOP to 62468\"                                                                                                1\n",
       ".;-):-D\"                                                                                                                                            1\n",
       "just been in bedbut mite go 2 thepub l8tr if uwana mt up?loads a luv Jenxxx.\\\"\"                                                                     1\n",
       " I'll come up\"                                                                                                                                      1\n",
       " just as a shop has to give a guarantee on what they sell. B. G.\"                                                                                   1\n",
       " But at d end my love compromised me for everything:-(\\\".. Gud mornin:-)\"                                                                           1\n",
       " smoke hella weed\\\"\"                                                                                                                                1\n",
       "Well there's still a bit left if you guys want to tonight                                                                                           1\n",
       "\\\" not \\\"what i need to do.\\\"\"                                                                                                                      1\n",
       "JUST GOT PAYED2DAY & I HAVBEEN GIVEN Aå£50 PAY RISE 4MY WORK & HAVEBEEN MADE PRESCHOOLCO-ORDINATOR 2I AM FEELINGOOD LUV\\\"\"                          1\n",
       " justthought iåÕd sayhey! how u doin?nearly the endof me wk offdam nevamind!We will have 2Hook up sn if uwant m8? loveJen x.\\\"\"                     1\n",
       "JUST REALLYNEED 2DOCD.PLEASE DONTPLEASE DONTIGNORE MYCALLS                                                                                          1\n",
       "u hav2hear it!c u sn xxxx\\\"\"                                                                                                                        1\n",
       " I don't mind                                                                                                                                       1\n",
       " the person is definitely special for u..... But if the person is so special                                                                        1\n",
       " ENJOYIN INDIANS AT THE MO..yeP. SaLL gOoD HehE ;> hows bout u shexy? Pete Xx\\\"\"                                                                    1\n",
       "Name: Unnamed: 2, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Unnamed: 2'].value_counts() #value count of unnamed: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02447f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " MK17 92H. 450Ppw 16\"                         2\n",
       "GE                                            2\n",
       " why to miss them                             1\n",
       "U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\"\"      1\n",
       "i wil tolerat.bcs ur my someone..... But      1\n",
       " ILLSPEAK 2 U2MORO WEN IM NOT ASLEEP...\\\"\"    1\n",
       "whoever is the KING\\\"!... Gud nyt\"            1\n",
       " TX 4 FONIN HON                               1\n",
       " \\\"OH No! COMPETITION\\\". Who knew             1\n",
       "IåÕL CALL U\\\"\"                                1\n",
       "Name: Unnamed: 3, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Unnamed: 3'].value_counts() \n",
    "#value counts for unnamed 3 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f75e590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNT:-)\"                                                     2\n",
       " just Keep-in-touch\\\" gdeve..\"                              1\n",
       " Never comfort me with a lie\\\" gud ni8 and sweet dreams\"    1\n",
       " CALL 2MWEN IM BK FRMCLOUD 9! J X\\\"\"                        1\n",
       " one day these two will become FREINDS FOREVER!\"            1\n",
       "Name: Unnamed: 4, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Unnamed: 4'].value_counts()\n",
    "#value counts for unnamed 4 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2a5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True) \n",
    "#droping those 3 columns which have high null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd0e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns ={'v1':'target',\"v2\":'independent'},inplace=True)\n",
    "#renaming the remaining columns from v1, v2 to target and independent respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685f084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                        independent\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #printing 5 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368dbf9",
   "metadata": {},
   "source": [
    "# data visualization\n",
    "#### this helps in understanding the data well so that we can do preprocessing and feature selection, extraction well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86de6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M RAJESH\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['target']) \n",
    "#printing the countplot for target column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816f622",
   "metadata": {},
   "source": [
    "here we can see that non spam values are more then spam values. and also the target variable has imbalance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2fd6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.swarmplot(x=df['independent'],y=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d39d55a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts:\u001b[0m\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "U dun say so early hor... U c already then say...\n",
      "Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts:\\033[0m\",*df[\"independent\"][:5], sep = \"\\n\") #first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e48daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Go until jurong point, crazy.. Available only ...\n",
      "1                           Ok lar... Joking wif u oni...\n",
      "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3       U dun say so early hor... U c already then say...\n",
      "4       Nah I don't think he goes to usf, he lives aro...\n",
      "                              ...                        \n",
      "5567    This is the 2nd time we have tried 2 contact u...\n",
      "5568                Will Ì_ b going to esplanade fr home?\n",
      "5569    Pity, * was in mood for that. So...any other s...\n",
      "5570    The guy did some bitching but I acted like i'd...\n",
      "5571                           Rofl. Its true to its name\n",
      "Name: independent, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['independent']) #some rows of independent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2006561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fine if thatåÕs the way u feel. ThatåÕs the way its gota b'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['independent'][18] #printing 19th column of independent column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5df6c",
   "metadata": {},
   "source": [
    "# converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f5c2003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                        independent\n",
       "0    ham  go until jurong point, crazy.. available only ...\n",
       "1    ham                      ok lar... joking wif u oni...\n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3    ham  u dun say so early hor... u c already then say...\n",
       "4    ham  nah i don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"independent\"] = df[\"independent\"].str.lower() #converting to lower case\n",
    "df.head() #printing 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e8452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts:\u001b[0m\n",
      "xxxmobilemovieclub: to use your credit, click the wap link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=qjkgighjjgcbl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts:\\033[0m\",*df[\"independent\"][15:16], sep = \"\\n\") #printing the rows of 15th column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc70f2",
   "metadata": {},
   "source": [
    "# Removing_html_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3d60dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?/\\>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb1ae07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['independent'] = df['independent'].apply(remove_html_tags) #applying html tag removal to independent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35b181c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 15 Texts:\u001b[0m\n",
      "xxxmobilemovieclub: to use your credit, click the wap link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=qjkgighjjgcbl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m\\u001b[45;1m The First 15 Texts:\\033[0m\",*df[\"independent\"][15:16], sep = \"\\n\") #first 15th row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b8219",
   "metadata": {},
   "source": [
    "# clean up the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9a4b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to clean up the text\n",
    "def Clean(Text):\n",
    "    sms = re.sub('[^a-zA-Z]', ' ', Text) #Replacing all non-alphabetic characters with a space\n",
    "    sms = sms.lower() #converting to lowecase\n",
    "    sms = sms.split() #spliting the text\n",
    "    sms = ' '.join(sms)\n",
    "    return sms\n",
    "\n",
    "df[\"independent\"] = df[\"independent\"].apply(Clean) #applying the clean function to the independent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fb9cab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts:\u001b[0m\n",
      "xxxmobilemovieclub to use your credit click the wap link in the next txt message or click here http wap xxxmobilemovieclub com n qjkgighjjgcbl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts:\\033[0m\",*df['independent'][15:16], sep = \"\\n\") #first 15th row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b6fdd",
   "metadata": {},
   "source": [
    "# Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "663e32ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in a wkly comp to win fa cup final ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don t think he goes to usf he lives arou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>freemsg hey there darling it s been week s now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>as per your request melle melle oru minnaminun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>winner as a valued network customer you have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>had your mobile months or more u r entitled to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>i m gonna be home soon and i don t want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>six chances to win cash from to pounds txt csh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>urgent you have won a week free membership in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>i ve been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>xxxmobilemovieclub to use your credit click th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>oh k i m watching here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>eh u remember how spell his name yes i did he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>fine if that s the way u feel that s the way i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>england v macedonia dont miss the goals team n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>is that seriously how you spell his name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>i m going to try for months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>so pay first lar then when is da stock comin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>aft i finish my lunch then i go str down lor a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>ffffffffff alright no way i can meet up with y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>just forced myself to eat a slice i m really n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>lol your always so convincing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>did you catch the bus are you frying an egg di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>i m back amp we re packing the car now i ll le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ham</td>\n",
       "      <td>ahhh work i vaguely remember that what does it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ham</td>\n",
       "      <td>wait that s still not all that clear were you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ham</td>\n",
       "      <td>yeah he got in at and was v apologetic n had f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ham</td>\n",
       "      <td>k tell me anything about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ham</td>\n",
       "      <td>for fear of fainting with the of all that hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>spam</td>\n",
       "      <td>thanks for your subscription to ringtone uk yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ham</td>\n",
       "      <td>yup ok i go home look at the timings then i ms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ham</td>\n",
       "      <td>oops i ll let you know when my roommate s done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ham</td>\n",
       "      <td>i see the letter b on my car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>ham</td>\n",
       "      <td>anything lor u decide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ham</td>\n",
       "      <td>hello how s you and how did saturday go i was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ham</td>\n",
       "      <td>pls go ahead with watts i just wanted to be su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ham</td>\n",
       "      <td>did i forget to tell you i want you i need you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>spam</td>\n",
       "      <td>rodger burns msg we tried to call you re your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ham</td>\n",
       "      <td>who are you seeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ham</td>\n",
       "      <td>great i hope you like your man well endowed i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ham</td>\n",
       "      <td>no calls messages missed calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ham</td>\n",
       "      <td>didn t you get hep b immunisation in nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ham</td>\n",
       "      <td>fair enough anything going on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ham</td>\n",
       "      <td>yeah hopefully if tyler can t do it i could ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ham</td>\n",
       "      <td>u don t know how stubborn i am i didn t even w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                        independent\n",
       "0     ham  go until jurong point crazy available only in ...\n",
       "1     ham                            ok lar joking wif u oni\n",
       "2    spam  free entry in a wkly comp to win fa cup final ...\n",
       "3     ham        u dun say so early hor u c already then say\n",
       "4     ham  nah i don t think he goes to usf he lives arou...\n",
       "5    spam  freemsg hey there darling it s been week s now...\n",
       "6     ham  even my brother is not like to speak with me t...\n",
       "7     ham  as per your request melle melle oru minnaminun...\n",
       "8    spam  winner as a valued network customer you have b...\n",
       "9    spam  had your mobile months or more u r entitled to...\n",
       "10    ham  i m gonna be home soon and i don t want to tal...\n",
       "11   spam  six chances to win cash from to pounds txt csh...\n",
       "12   spam  urgent you have won a week free membership in ...\n",
       "13    ham  i ve been searching for the right words to tha...\n",
       "14    ham                  i have a date on sunday with will\n",
       "15   spam  xxxmobilemovieclub to use your credit click th...\n",
       "16    ham                             oh k i m watching here\n",
       "17    ham  eh u remember how spell his name yes i did he ...\n",
       "18    ham  fine if that s the way u feel that s the way i...\n",
       "19   spam  england v macedonia dont miss the goals team n...\n",
       "20    ham           is that seriously how you spell his name\n",
       "21    ham      i m going to try for months ha ha only joking\n",
       "22    ham       so pay first lar then when is da stock comin\n",
       "23    ham  aft i finish my lunch then i go str down lor a...\n",
       "24    ham  ffffffffff alright no way i can meet up with y...\n",
       "25    ham  just forced myself to eat a slice i m really n...\n",
       "26    ham                      lol your always so convincing\n",
       "27    ham  did you catch the bus are you frying an egg di...\n",
       "28    ham  i m back amp we re packing the car now i ll le...\n",
       "29    ham  ahhh work i vaguely remember that what does it...\n",
       "30    ham  wait that s still not all that clear were you ...\n",
       "31    ham  yeah he got in at and was v apologetic n had f...\n",
       "32    ham                       k tell me anything about you\n",
       "33    ham  for fear of fainting with the of all that hous...\n",
       "34   spam  thanks for your subscription to ringtone uk yo...\n",
       "35    ham  yup ok i go home look at the timings then i ms...\n",
       "36    ham     oops i ll let you know when my roommate s done\n",
       "37    ham                       i see the letter b on my car\n",
       "38    ham                              anything lor u decide\n",
       "39    ham  hello how s you and how did saturday go i was ...\n",
       "40    ham  pls go ahead with watts i just wanted to be su...\n",
       "41    ham  did i forget to tell you i want you i need you...\n",
       "42   spam  rodger burns msg we tried to call you re your ...\n",
       "43    ham                                 who are you seeing\n",
       "44    ham  great i hope you like your man well endowed i ...\n",
       "45    ham                     no calls messages missed calls\n",
       "46    ham       didn t you get hep b immunisation in nigeria\n",
       "47    ham                      fair enough anything going on\n",
       "48    ham  yeah hopefully if tyler can t do it i could ma...\n",
       "49    ham  u don t know how stubborn i am i didn t even w..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df[\"independent\"] = df[\"independent\"].apply(lambda text: remove_punctuation(text)) #applying the punctuation removal function\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "453cc5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[45;1m The First 5 Texts:\u001b[0m\n",
      "did you hear about the new divorce barbie it comes with all of ken s stuff\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1m\\u001b[45;1m The First 5 Texts:\\033[0m\",*df['independent'][68:69], sep = \"\\n\") #printing the particular row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde83a46",
   "metadata": {},
   "source": [
    "punctuations are removed from the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cc49a",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbbd93c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                        independent\n",
       "0    ham  go jurong point crazy available bugis n great ...\n",
       "1    ham                            ok lar joking wif u oni\n",
       "2   spam  free entry wkly comp win fa cup final tkts st ...\n",
       "3    ham                u dun say early hor u c already say\n",
       "4    ham             nah think goes usf lives around though"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"independent\"] = df[\"independent\"].apply(lambda text: remove_stopwords(text)) #applying the removal of stopword function from independent column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f16af",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "It is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39954281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                        independent\n",
       "0    ham  go jurong point crazi avail bugi n great world...\n",
       "1    ham                              ok lar joke wif u oni\n",
       "2   spam  free entri wkli comp win fa cup final tkt st m...\n",
       "3    ham                u dun say earli hor u c alreadi say\n",
       "4    ham               nah think goe usf live around though"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "df[\"independent\"] = df[\"independent\"].apply(lambda text: stem_words(text)) #applying stemming to the independent column\n",
    "df.head() #printing 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae2dca5",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "def lemmatize_word(text):\n",
    "    #word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas\n",
    "\n",
    "data[\"Lemmatized_Text\"] = df[\"independent\"].apply(lemmatize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "142ba57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>nd time tri contact u u pound prize claim easi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>piti mood suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>guy bitch act like interest buy someth els nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                        independent\n",
       "0       ham  go jurong point crazi avail bugi n great world...\n",
       "1       ham                              ok lar joke wif u oni\n",
       "2      spam  free entri wkli comp win fa cup final tkt st m...\n",
       "3       ham                u dun say earli hor u c alreadi say\n",
       "4       ham               nah think goe usf live around though\n",
       "...     ...                                                ...\n",
       "5567   spam  nd time tri contact u u pound prize claim easi...\n",
       "5568    ham                              b go esplanad fr home\n",
       "5569    ham                                  piti mood suggest\n",
       "5570    ham  guy bitch act like interest buy someth els nex...\n",
       "5571    ham                                     rofl true name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b76b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #count vectorizer library is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "196ec543",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69ce656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= cv.fit_transform(df['independent']) #fitting and transforming the independent column into count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02036370",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.toarray() #converting to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8109debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aah',\n",
       " 'aaniy',\n",
       " 'aaooooright',\n",
       " 'aathi',\n",
       " 'ab',\n",
       " 'abbey',\n",
       " 'abdomen',\n",
       " 'abeg',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'abi',\n",
       " 'abil',\n",
       " 'abiola',\n",
       " 'abj',\n",
       " 'abl',\n",
       " 'abnorm',\n",
       " 'abouta',\n",
       " 'abroad',\n",
       " 'absenc',\n",
       " 'absolut',\n",
       " 'absolutli',\n",
       " 'abstract',\n",
       " 'abt',\n",
       " 'abta',\n",
       " 'aburo',\n",
       " 'abus',\n",
       " 'ac',\n",
       " 'academ',\n",
       " 'acc',\n",
       " 'accent',\n",
       " 'accentur',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accid',\n",
       " 'accident',\n",
       " 'accommod',\n",
       " 'accommodationvouch',\n",
       " 'accomod',\n",
       " 'accordin',\n",
       " 'accordingli',\n",
       " 'account',\n",
       " 'accumul',\n",
       " 'ach',\n",
       " 'achan',\n",
       " 'achiev',\n",
       " 'acid',\n",
       " 'acknowledg',\n",
       " 'acl',\n",
       " 'acnt',\n",
       " 'aco',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actin',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'addamsfa',\n",
       " 'addi',\n",
       " 'addict',\n",
       " 'address',\n",
       " 'adewal',\n",
       " 'adi',\n",
       " 'adjust',\n",
       " 'admin',\n",
       " 'administr',\n",
       " 'admir',\n",
       " 'admiss',\n",
       " 'admit',\n",
       " 'ador',\n",
       " 'adp',\n",
       " 'adress',\n",
       " 'adrian',\n",
       " 'adsens',\n",
       " 'adult',\n",
       " 'advanc',\n",
       " 'adventur',\n",
       " 'advic',\n",
       " 'advis',\n",
       " 'advisor',\n",
       " 'ae',\n",
       " 'aeronaut',\n",
       " 'aeroplan',\n",
       " 'afew',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affection',\n",
       " 'affidavit',\n",
       " 'afford',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'afternon',\n",
       " 'afternoon',\n",
       " 'afterward',\n",
       " 'aftr',\n",
       " 'ag',\n",
       " 'agalla',\n",
       " 'age',\n",
       " 'agenc',\n",
       " 'agent',\n",
       " 'agidhan',\n",
       " 'ago',\n",
       " 'agre',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahmad',\n",
       " 'ahold',\n",
       " 'aid',\n",
       " 'aig',\n",
       " 'aight',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'aiya',\n",
       " 'aiyah',\n",
       " 'aiyar',\n",
       " 'aiyo',\n",
       " 'aj',\n",
       " 'ajith',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akon',\n",
       " 'al',\n",
       " 'alaikkum',\n",
       " 'alaipayuth',\n",
       " 'albi',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'aldrin',\n",
       " 'alert',\n",
       " 'alertfrom',\n",
       " 'alett',\n",
       " 'alex',\n",
       " 'alfi',\n",
       " 'algarv',\n",
       " 'algebra',\n",
       " 'algorithm',\n",
       " 'ali',\n",
       " 'alian',\n",
       " 'alibi',\n",
       " 'aliv',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'allalo',\n",
       " 'allday',\n",
       " 'allo',\n",
       " 'allow',\n",
       " 'almost',\n",
       " 'alon',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alreadi',\n",
       " 'alright',\n",
       " 'alrit',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'altern',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'aluabl',\n",
       " 'alwa',\n",
       " 'alway',\n",
       " 'alwi',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amaz',\n",
       " 'ambiti',\n",
       " 'ambrith',\n",
       " 'american',\n",
       " 'ami',\n",
       " 'amigo',\n",
       " 'amk',\n",
       " 'amla',\n",
       " 'amma',\n",
       " 'ammo',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amor',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amplikat',\n",
       " 'amrca',\n",
       " 'amrita',\n",
       " 'amt',\n",
       " 'amus',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'analysi',\n",
       " 'anand',\n",
       " 'anderson',\n",
       " 'andr',\n",
       " 'andrew',\n",
       " 'andro',\n",
       " 'angel',\n",
       " 'angri',\n",
       " 'anim',\n",
       " 'anji',\n",
       " 'anjola',\n",
       " 'anna',\n",
       " 'anni',\n",
       " 'anniversari',\n",
       " 'annonc',\n",
       " 'announc',\n",
       " 'annoy',\n",
       " 'annoyin',\n",
       " 'anonym',\n",
       " 'anot',\n",
       " 'anoth',\n",
       " 'ansr',\n",
       " 'answer',\n",
       " 'answerin',\n",
       " 'answr',\n",
       " 'antelop',\n",
       " 'antha',\n",
       " 'anthoni',\n",
       " 'anti',\n",
       " 'antibiot',\n",
       " 'anybodi',\n",
       " 'anyhow',\n",
       " 'anymor',\n",
       " 'anyon',\n",
       " 'anyplac',\n",
       " 'anyth',\n",
       " 'anythi',\n",
       " 'anythin',\n",
       " 'anythingtomorrow',\n",
       " 'anytim',\n",
       " 'anyway',\n",
       " 'anywher',\n",
       " 'aom',\n",
       " 'apart',\n",
       " 'ape',\n",
       " 'apeshit',\n",
       " 'aphex',\n",
       " 'apnt',\n",
       " 'apo',\n",
       " 'apolog',\n",
       " 'apologet',\n",
       " 'apologis',\n",
       " 'app',\n",
       " 'appar',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appendix',\n",
       " 'appi',\n",
       " 'appl',\n",
       " 'applausestor',\n",
       " 'applebe',\n",
       " 'appli',\n",
       " 'applic',\n",
       " 'appoint',\n",
       " 'appreci',\n",
       " 'approach',\n",
       " 'appropri',\n",
       " 'approv',\n",
       " 'approx',\n",
       " 'appt',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'aproach',\n",
       " 'apt',\n",
       " 'aptitud',\n",
       " 'aq',\n",
       " 'aquariu',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabian',\n",
       " 'arcad',\n",
       " 'archiv',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arestaur',\n",
       " 'aretak',\n",
       " 'areyouuniqu',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'argu',\n",
       " 'argument',\n",
       " 'ari',\n",
       " 'aris',\n",
       " 'arithmet',\n",
       " 'arm',\n",
       " 'armand',\n",
       " 'armenia',\n",
       " 'arng',\n",
       " 'arngd',\n",
       " 'arnt',\n",
       " 'around',\n",
       " 'aroundn',\n",
       " 'arr',\n",
       " 'arrang',\n",
       " 'arrest',\n",
       " 'arriv',\n",
       " 'arrow',\n",
       " 'arsen',\n",
       " 'art',\n",
       " 'arti',\n",
       " 'artist',\n",
       " 'arul',\n",
       " 'arun',\n",
       " 'asa',\n",
       " 'asap',\n",
       " 'asda',\n",
       " 'ash',\n",
       " 'ashley',\n",
       " 'ashwini',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'asjesu',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'askin',\n",
       " 'aslamalaikkum',\n",
       " 'asleep',\n",
       " 'asp',\n",
       " 'aspect',\n",
       " 'ass',\n",
       " 'assess',\n",
       " 'asshol',\n",
       " 'assist',\n",
       " 'associ',\n",
       " 'asssssholeee',\n",
       " 'assum',\n",
       " 'asther',\n",
       " 'asthma',\n",
       " 'astn',\n",
       " 'astoundingli',\n",
       " 'astrolog',\n",
       " 'astronom',\n",
       " 'asu',\n",
       " 'asusu',\n",
       " 'ate',\n",
       " 'athlet',\n",
       " 'athom',\n",
       " 'atlanta',\n",
       " 'atlast',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atroci',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'atten',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'attitud',\n",
       " 'attract',\n",
       " 'attribut',\n",
       " 'atur',\n",
       " 'auction',\n",
       " 'audiit',\n",
       " 'audit',\n",
       " 'audrey',\n",
       " 'audri',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'aunti',\n",
       " 'aust',\n",
       " 'australia',\n",
       " 'authoris',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'avail',\n",
       " 'availa',\n",
       " 'avalarr',\n",
       " 'avatar',\n",
       " 'avbl',\n",
       " 'ave',\n",
       " 'aveng',\n",
       " 'avent',\n",
       " 'avenu',\n",
       " 'avin',\n",
       " 'avo',\n",
       " 'avoid',\n",
       " 'await',\n",
       " 'awak',\n",
       " 'award',\n",
       " 'away',\n",
       " 'awesom',\n",
       " 'awkward',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'ax',\n",
       " 'axel',\n",
       " 'axi',\n",
       " 'ay',\n",
       " 'ayn',\n",
       " 'ayo',\n",
       " 'ba',\n",
       " 'baaaaaaaab',\n",
       " 'baaaaab',\n",
       " 'babe',\n",
       " 'babi',\n",
       " 'babygoodby',\n",
       " 'babyjontet',\n",
       " 'babysit',\n",
       " 'bac',\n",
       " 'back',\n",
       " 'backdoor',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badli',\n",
       " 'badrith',\n",
       " 'bag',\n",
       " 'bahama',\n",
       " 'baig',\n",
       " 'bailiff',\n",
       " 'bajarangabali',\n",
       " 'bak',\n",
       " 'bakra',\n",
       " 'bakrid',\n",
       " 'balanc',\n",
       " 'ball',\n",
       " 'baller',\n",
       " 'balloon',\n",
       " 'bam',\n",
       " 'bambl',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandag',\n",
       " 'bang',\n",
       " 'bangb',\n",
       " 'bangbab',\n",
       " 'bani',\n",
       " 'bank',\n",
       " 'banneduk',\n",
       " 'banter',\n",
       " 'bao',\n",
       " 'bar',\n",
       " 'barbi',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'bari',\n",
       " 'barkley',\n",
       " 'barm',\n",
       " 'barolla',\n",
       " 'barrel',\n",
       " 'barri',\n",
       " 'base',\n",
       " 'bash',\n",
       " 'basic',\n",
       " 'basket',\n",
       " 'basketbal',\n",
       " 'basq',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'batchlor',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'batsman',\n",
       " 'batt',\n",
       " 'batteri',\n",
       " 'battl',\n",
       " 'bawl',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbd',\n",
       " 'bbdelux',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bcaz',\n",
       " 'bck',\n",
       " 'bcm',\n",
       " 'bcmsfwc',\n",
       " 'bcoz',\n",
       " 'bcum',\n",
       " 'bcz',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'bead',\n",
       " 'bear',\n",
       " 'beat',\n",
       " 'beauti',\n",
       " 'bec',\n",
       " 'becau',\n",
       " 'becausethey',\n",
       " 'becom',\n",
       " 'becoz',\n",
       " 'becz',\n",
       " 'bed',\n",
       " 'bedrm',\n",
       " 'bedroom',\n",
       " 'beeen',\n",
       " 'beehoon',\n",
       " 'beendrop',\n",
       " 'beer',\n",
       " 'beerag',\n",
       " 'befor',\n",
       " 'beforehand',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'begin',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behav',\n",
       " 'behind',\n",
       " 'bein',\n",
       " 'believ',\n",
       " 'beliv',\n",
       " 'bell',\n",
       " 'bellearli',\n",
       " 'belli',\n",
       " 'belliger',\n",
       " 'belong',\n",
       " 'belov',\n",
       " 'belovd',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bend',\n",
       " 'beneath',\n",
       " 'beneficiari',\n",
       " 'benefit',\n",
       " 'benni',\n",
       " 'bergkamp',\n",
       " 'besid',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'beth',\n",
       " 'betta',\n",
       " 'better',\n",
       " 'bettersn',\n",
       " 'bettr',\n",
       " 'beverag',\n",
       " 'bevi',\n",
       " 'bewar',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'bfore',\n",
       " 'bhaji',\n",
       " 'bhamb',\n",
       " 'bhaskar',\n",
       " 'bhayandar',\n",
       " 'bian',\n",
       " 'biatch',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billi',\n",
       " 'billion',\n",
       " 'bilo',\n",
       " 'bimbo',\n",
       " 'bin',\n",
       " 'biola',\n",
       " 'bird',\n",
       " 'birla',\n",
       " 'biro',\n",
       " 'birth',\n",
       " 'birthdat',\n",
       " 'birthday',\n",
       " 'bishan',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'biz',\n",
       " 'bk',\n",
       " 'black',\n",
       " 'blackberri',\n",
       " 'blacko',\n",
       " 'blah',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blastin',\n",
       " 'bleak',\n",
       " 'bleh',\n",
       " 'bless',\n",
       " 'blimey',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'blogspot',\n",
       " 'bloke',\n",
       " 'blond',\n",
       " 'bloo',\n",
       " 'blood',\n",
       " 'bloodi',\n",
       " 'bloomberg',\n",
       " 'blow',\n",
       " 'blown',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'bluetooth',\n",
       " 'bluetoothhdset',\n",
       " 'bluff',\n",
       " 'blur',\n",
       " 'bluray',\n",
       " 'bmw',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'boatin',\n",
       " 'bob',\n",
       " 'bodi',\n",
       " 'boggi',\n",
       " 'bognor',\n",
       " 'bold',\n",
       " 'bollox',\n",
       " 'boltblu',\n",
       " 'bomb',\n",
       " 'bone',\n",
       " 'bong',\n",
       " 'bonu',\n",
       " 'boo',\n",
       " 'boob',\n",
       " 'book',\n",
       " 'bookedth',\n",
       " 'bookmark',\n",
       " 'bookshelf',\n",
       " 'boooo',\n",
       " 'boost',\n",
       " 'booti',\n",
       " 'bootydeli',\n",
       " 'borderlin',\n",
       " 'bore',\n",
       " 'borin',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bottl',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'boundari',\n",
       " 'bout',\n",
       " 'bowa',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyf',\n",
       " 'boyfriend',\n",
       " 'boytoy',\n",
       " 'bp',\n",
       " 'bpo',\n",
       " 'bra',\n",
       " 'brah',\n",
       " 'brain',\n",
       " 'braindanc',\n",
       " 'braini',\n",
       " 'brainless',\n",
       " 'brand',\n",
       " 'brandi',\n",
       " 'brat',\n",
       " 'brave',\n",
       " 'bray',\n",
       " 'brb',\n",
       " 'brdget',\n",
       " 'bread',\n",
       " 'breadstick',\n",
       " 'break',\n",
       " 'breaker',\n",
       " 'breakfast',\n",
       " 'breakin',\n",
       " 'breath',\n",
       " 'breather',\n",
       " 'breez',\n",
       " 'breezi',\n",
       " 'bremov',\n",
       " 'bribe',\n",
       " 'bridal',\n",
       " 'bridg',\n",
       " 'bridgwat',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brilliant',\n",
       " 'brilliantli',\n",
       " 'brin',\n",
       " 'bring',\n",
       " 'brisk',\n",
       " 'brison',\n",
       " 'bristol',\n",
       " 'british',\n",
       " 'britney',\n",
       " 'bro',\n",
       " 'broad',\n",
       " 'broadband',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brolli',\n",
       " 'broth',\n",
       " 'brotha',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'browni',\n",
       " 'brows',\n",
       " 'browser',\n",
       " 'browsin',\n",
       " 'bruce',\n",
       " 'brum',\n",
       " 'bruv',\n",
       " 'bslvyl',\n",
       " 'bsn',\n",
       " 'bsnl',\n",
       " 'bstfrnd',\n",
       " 'bt',\n",
       " 'btw',\n",
       " 'btwn',\n",
       " 'bu',\n",
       " 'bubbletext',\n",
       " 'buck',\n",
       " 'bud',\n",
       " 'buddi',\n",
       " 'budget',\n",
       " 'buen',\n",
       " 'buff',\n",
       " 'buffet',\n",
       " 'buffi',\n",
       " 'bugi',\n",
       " 'build',\n",
       " 'built',\n",
       " 'bulb',\n",
       " 'bull',\n",
       " 'bullshit',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'bundl',\n",
       " 'bunker',\n",
       " 'burden',\n",
       " 'burger',\n",
       " 'burgundi',\n",
       " 'burial',\n",
       " 'burn',\n",
       " 'burnt',\n",
       " 'burrito',\n",
       " 'buse',\n",
       " 'busetop',\n",
       " 'busi',\n",
       " 'busti',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'butther',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buzi',\n",
       " 'buzz',\n",
       " 'buzzzz',\n",
       " 'bw',\n",
       " 'bx',\n",
       " 'byatch',\n",
       " 'bye',\n",
       " 'cab',\n",
       " 'cabin',\n",
       " 'cabl',\n",
       " 'cafe',\n",
       " 'cage',\n",
       " 'cake',\n",
       " 'cal',\n",
       " 'calcul',\n",
       " 'cali',\n",
       " 'calicut',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callback',\n",
       " 'callcost',\n",
       " 'calld',\n",
       " 'caller',\n",
       " 'callertun',\n",
       " 'callfreefon',\n",
       " 'callin',\n",
       " 'callon',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'camcord',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campu',\n",
       " 'camri',\n",
       " 'canada',\n",
       " 'canal',\n",
       " 'canari',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'canlov',\n",
       " 'cann',\n",
       " 'cannam',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cantdo',\n",
       " 'canteen',\n",
       " 'cap',\n",
       " 'capac',\n",
       " 'capit',\n",
       " 'cappuccino',\n",
       " 'captain',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardiff',\n",
       " 'cardin',\n",
       " 'care',\n",
       " 'careabout',\n",
       " 'career',\n",
       " 'careless',\n",
       " 'carli',\n",
       " 'carlin',\n",
       " 'carlo',\n",
       " 'carolin',\n",
       " 'carolina',\n",
       " 'carpark',\n",
       " 'carri',\n",
       " 'carryin',\n",
       " 'carton',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cashbin',\n",
       " 'cashto',\n",
       " 'cast',\n",
       " 'castor',\n",
       " 'casualti',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'categori',\n",
       " 'caught',\n",
       " 'caus',\n",
       " 'cave',\n",
       " 'caveboy',\n",
       " 'cbe',\n",
       " 'cc',\n",
       " 'ccna',\n",
       " 'cd',\n",
       " 'cdgt',\n",
       " 'cedar',\n",
       " 'ceil',\n",
       " 'celeb',\n",
       " 'celebr',\n",
       " 'cell',\n",
       " 'censu',\n",
       " 'center',\n",
       " 'centr',\n",
       " 'centuri',\n",
       " 'cer',\n",
       " 'cereal',\n",
       " 'ceri',\n",
       " 'certainli',\n",
       " 'certif',\n",
       " 'cfca',\n",
       " 'ch',\n",
       " 'cha',\n",
       " 'chachi',\n",
       " 'chad',\n",
       " 'chain',\n",
       " 'challeng',\n",
       " 'champ',\n",
       " 'champlaxig',\n",
       " 'champney',\n",
       " 'chanc',\n",
       " 'chang',\n",
       " 'channel',\n",
       " 'chap',\n",
       " 'chapel',\n",
       " 'chapter',\n",
       " 'charact',\n",
       " 'charg',\n",
       " 'chariti',\n",
       " 'charl',\n",
       " 'charli',\n",
       " 'charm',\n",
       " 'chart',\n",
       " 'chase',\n",
       " 'chastiti',\n",
       " 'chat',\n",
       " 'chatlin',\n",
       " 'chatter',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheat',\n",
       " 'chechi',\n",
       " 'check',\n",
       " 'checkbox',\n",
       " 'checkin',\n",
       " 'checkmat',\n",
       " 'checkup',\n",
       " 'cheek',\n",
       " 'cheer',\n",
       " 'cheeri',\n",
       " 'chees',\n",
       " 'cheesi',\n",
       " 'cheeto',\n",
       " 'chef',\n",
       " 'chennai',\n",
       " 'chequ',\n",
       " 'cherish',\n",
       " 'cherthala',\n",
       " 'chess',\n",
       " 'chest',\n",
       " 'chex',\n",
       " 'cheyyamo',\n",
       " 'chez',\n",
       " 'chg',\n",
       " 'chic',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'chief',\n",
       " 'chik',\n",
       " 'chikku',\n",
       " 'child',\n",
       " 'childish',\n",
       " 'childporn',\n",
       " 'children',\n",
       " 'chile',\n",
       " 'chill',\n",
       " 'chillaxin',\n",
       " 'chillin',\n",
       " 'china',\n",
       " 'chinatown',\n",
       " 'chinchilla',\n",
       " 'chines',\n",
       " 'chinki',\n",
       " 'chinnu',\n",
       " 'chiong',\n",
       " 'chip',\n",
       " 'chit',\n",
       " 'chk',\n",
       " 'chloe',\n",
       " 'chocol',\n",
       " 'choic',\n",
       " 'choos',\n",
       " 'chop',\n",
       " 'chord',\n",
       " 'chore',\n",
       " 'chosen',\n",
       " 'chrgd',\n",
       " 'christ',\n",
       " 'christian',\n",
       " 'christma',\n",
       " 'christmassi',\n",
       " 'chuck',\n",
       " 'chuckin',\n",
       " 'church',\n",
       " 'ciao',\n",
       " 'cine',\n",
       " 'cinema',\n",
       " 'citi',\n",
       " 'citizen',\n",
       " 'citylink',\n",
       " 'cktz',\n",
       " 'cl',\n",
       " 'cla',\n",
       " 'claim',\n",
       " 'claimcod',\n",
       " 'clair',\n",
       " 'clarif',\n",
       " 'clarifi',\n",
       " 'clark',\n",
       " 'clash',\n",
       " 'class',\n",
       " 'classic',\n",
       " 'classmat',\n",
       " 'claypot',\n",
       " 'cld',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clearer',\n",
       " 'clearli',\n",
       " 'clever',\n",
       " 'click',\n",
       " 'cliff',\n",
       " 'clip',\n",
       " 'clo',\n",
       " 'clock',\n",
       " 'close',\n",
       " 'closebi',\n",
       " 'closer',\n",
       " 'closingd',\n",
       " 'cloth',\n",
       " 'cloud',\n",
       " 'clover',\n",
       " 'club',\n",
       " 'clubmobi',\n",
       " 'clubsaisai',\n",
       " 'clubz',\n",
       " 'clue',\n",
       " 'cm',\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cv.vocabulary_.keys()) #checking vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7262ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=pd.get_dummies(df['target'],drop_first=True)\n",
    "#using getdummies to encode the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9497e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                        independent\n",
       "0       0  go jurong point crazi avail bugi n great world...\n",
       "1       0                              ok lar joke wif u oni\n",
       "2       1  free entri wkli comp win fa cup final tkt st m...\n",
       "3       0                u dun say earli hor u c alreadi say\n",
       "4       0               nah think goe usf live around though"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #printing 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "532f8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize #importing word tokenzier library\n",
    "df['independent'] = df.independent.apply(word_tokenize) #applying work tokenzie library to independent column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76bebd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>independent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                        independent\n",
       "0       0  [go, jurong, point, crazi, avail, bugi, n, gre...\n",
       "1       0                       [ok, lar, joke, wif, u, oni]\n",
       "2       1  [free, entri, wkli, comp, win, fa, cup, final,...\n",
       "3       0      [u, dun, say, earli, hor, u, c, alreadi, say]\n",
       "4       0       [nah, think, goe, usf, live, around, though]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #printing 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba10aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df[\"target\"]  \n",
    "# Splitting the testing and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50a2f10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 6221)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape #rows and columns of x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ed4721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape#rows and columns of y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad2018",
   "metadata": {},
   "source": [
    "# Model building using Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023e90d",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8dd9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model accuracy(in %): 97.75784753363229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reg =LogisticRegression()\n",
    "  \n",
    "# train the model using the training sets\n",
    "reg.fit(x_train, y_train)\n",
    " \n",
    "# making predictions on the testing set\n",
    "y_pred = reg.predict(x_test)\n",
    "  \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "print(\"Logistic Regression model accuracy(in %):\",\n",
    "metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4835bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_score, recall_score, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ac24827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[965,   0],\n",
       "       [ 25, 125]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "377a59e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9c5cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       965\n",
      "           1       1.00      0.83      0.91       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9cd84",
   "metadata": {},
   "source": [
    "## NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b8e9167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 86.27802690582959\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "  \n",
    "# making predictions on the testing set\n",
    "y_pred = gnb.predict(x_test)\n",
    "  \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "from sklearn import metrics\n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9dfeac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[828, 137],\n",
       "       [ 16, 134]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187766d5",
   "metadata": {},
   "source": [
    "## RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9900e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian rf model accuracy(in %): 97.04035874439462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "classifier= RandomForestClassifier(n_estimators= 10, criterion=\"gini\")  \n",
    "classifier.fit(x_train, y_train)  \n",
    "# making predictions on the testing set\n",
    "y_pred = classifier.predict(x_test)\n",
    "from sklearn import metrics\n",
    "print(\"Gaussian rf model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a4b9876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[965,   0],\n",
       "       [ 33, 117]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8aec82",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1c61e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " knn model accuracy(in %): 91.92825112107623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "  \n",
    "knn.fit(x_train, y_train)\n",
    "  \n",
    "# Predict on dataset which model has not seen before\n",
    "y_pred=knn.predict(x_test)\n",
    "\n",
    "print(\" knn model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e89d6",
   "metadata": {},
   "source": [
    "## support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c28d89d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " svc model accuracy(in %): 97.75784753363229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svc=SVC()\n",
    "  \n",
    "svc.fit(x_train, y_train)\n",
    "  \n",
    "# Predict on dataset which model has not seen before\n",
    "y_pred=svc.predict(x_test)\n",
    "\n",
    "print(\" svc model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae49e9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[965,   0],\n",
       "       [ 25, 125]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd280f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0995a49",
   "metadata": {},
   "source": [
    "#logisticRegression gives highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2347e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
